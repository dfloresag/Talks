<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Bootstrapping Clustered Data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Daniel Flores-Agreda" />
    <link href="libs/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
    <link href="libs/academicons-1.8.6/css/academicons.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/monash.css" type="text/css" />
    <link rel="stylesheet" href="css/monash-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Bootstrapping Clustered Data
## via a Weighted Laplace Approximation
### Daniel Flores-Agreda
### School of Public Health and Preventive Medicine
### 2/12/2019

---

class: center, middle

# Joint Work

---
class: center, middle

# _(Co)-Starring:_

---
class: center

# _(Co)-Starring:_
.column[

### Eva Cantoni

&lt;img src="https://www.unige.ch/gsem/files/cache/89437c652682ce133175f82b3238bb0a_f3835.jpg" width="350"&gt;

&lt;img src="./img/gsem_en.png" width="150"&gt;

]

.column[

### Stephane Heritier

&lt;img src="https://research.monash.edu/files-asset/248358259/Stephane_Roland_Heritier.jpg" width="150"&gt;

&lt;img src="./img/monash-stacked-black-cmyk.png" width="150"&gt;

]

.column[

### Rory Wolfe

&lt;img src="https://research.monash.edu/files-asset/248351697/profile+photo" width="150"&gt;

&lt;img src="./img/monash-stacked-black-cmyk.png" width="150"&gt;

]



---


# Outline

1. **Clustered Data**

2. **Bootstrapping** Clustered Data

3. Bootstrapping Clustered Data via a **Weighted Laplace Approximation**

4. (Some) Properties

5. Discussion and Current Work

---
class: inverse, center, middle







&lt;!-- Tell a story  --&gt;
&lt;!-- This is the journey : Elevator pitch 30 seconds --&gt;
&lt;!-- Signpost along the way, i.e. 
&lt;!-- Say where are we going and where are we up to in the story --&gt;

# Clustered Data

---
class: center

### Data that can be **grouped**, according to a **hierarchical** structure. 

&lt;img src="./img/Charts-001.png" width="600"&gt;

### _Highest_ level in the hierarchy: Independent **Clusters** ( `\(i\)` )

???



A natural clustering factor in medical applications and longitudinal studies is the patient level. from which the different observations come. 


In this case, the patient, but in general it can be other things, such as hospitals in centre-patient studies. More levels of clustering can be considered.
---
class: center

### Modelling `\(\Rightarrow\)` **Mixed Models** (MM)

.content-box-lightgrey[
`$$\text{Mixed Effects} = \textbf{Fixed Effects} + \color{blue}{\textbf{Random Effects}}$$`
]

---
class: center

### Modelling `\(\Rightarrow\)` **Mixed Models** (MM)

.content-box-lightgrey[
`$$\text{Mixed Effects} = \underbrace{\textbf{Fixed Effects}}_{\mathbf{X}_{i}\mathbf{\beta}} + \underbrace{\color{blue}{\textbf{Random Effects}}}_{\color{blue}{\mathbf{Z}_{i}\mathbf{D}_{\sigma}\mathbf{u}_{i}}}$$`
]

--
.pull-left[
Generalized Linear Mixed Models (**GLMM**)

.content-box-lightgrey[
`$${\scriptsize \underbrace{\text{link function}}_{g}\underbrace{\left[\mathbb{E}\text{xpected}\left(\textbf{Outcomes}\right)\right]}_{\left[\mathbb{E}\left( \mathbf{Y}_{i} \ \vert \ \mathbf{X}_{i} , \mathbf{u}_{i} \right)\right]} = \mathbf{X}_{i}\mathbf{\beta} + \color{blue}{\mathbf{Z}_{i}\mathbf{D}_{\sigma}\mathbf{u}_{i}}}$$`
]

.black[Example] Longitudinal **Mixed Logit**: 

.content-box-lightgrey[

`$$\log \frac{p_{ij}}{1-p_{ij}} = \mathbf{x}_{ij}^{T}\mathbf{\beta} + \color{blue}{\sigma_{1} u_{i}}$$`
]
]

--
.pull-right[

Accelerated Failure Time Models (**AFT**)

.content-box-lightgrey[

`$${\scriptsize \text{log}\underbrace{\left(\textbf{Duration}\right)}_{ \mathbf{T}_{i} } = \mathbf{X}_{i}\mathbf{\beta}+\color{blue}{\mathbf{Z}_{i}\mathbf{D}_{\sigma}\mathbf{u}_{i}}+\color{purple}{\phi\mathbf{\varepsilon}_i}}$$`
]

.black[Example]: **AFT** with **Random Intercept** and **Right-Censoring**

.content-box-lightgrey[
`\begin{align}
\log t_{ij}&amp;= \mathbf{x}_{it}^T\mathbf{\beta} + \color{blue}{\sigma_1 u_i} + \phi\varepsilon_{it}\\
Y_{ij} &amp;= (1-\delta_{ij}) t_{ij}+\delta_{ij} C_{ij}
\end{align}`
`\(\delta_{ij}\)`, `\(C_{ij}\)`: censoring **indicators** and **times**
]
]

???

For parsimony, the effects of the different levels in this hierarchy are considered as random, this has the added advantage of modelling within-cluster correlations.

Mixed models _link_ the expected outcome with a combination of Fixed Effects and Random effects. 

This combination can be _linear_ yielding the class of generalized Linear Mixed Models. 

---
class: center, middle

### Likelihood Inference `\(\Rightarrow\)` **(Numerical) Integration** (of `\(\color{\blue}{\mathbf{u}_{i}}\)` )

.content-box-lightgrey[
`$$\color{red}{\mathcal{L}_i\left(\mathbf{\theta}\right)} = \color{red}{f\left(\mathbf{y}_{i}\right)}$$`
]

---
class: center, middle

### Likelihood Inference `\(\Rightarrow\)` **(Numerical) Integration** (of `\(\color{\blue}{\mathbf{u}_{i}}\)` )

.content-box-lightgrey[
`$$\color{red}{\mathcal{L}_i\left(\mathbf{\theta}\right)} = \color{blue}{\int_{\mathbb{R}^q}} f\left(\mathbf{y}_i, \color{blue}{\mathbf{u}_i}\right) \mathrm{d} \color{blue}{\mathbf{u}_{i}}$$`
]

???

As with many problems with unobserved random effects, these need to be partialed out by integration from the joint pdf of responses and random effects in order to construct the likelihood. 

---
class: center, middle

### Likelihood Inference `\(\Rightarrow\)` **(Numerical) Integration** (of `\(\color{\blue}{\mathbf{u}_{i}}\)` )

.content-box-lightgrey[
`$$\color{red}{\mathcal{L}_i\left(\mathbf{\theta}\right)} = \int_{\mathbb{R}^q} \color{blue}{f\left(\mathbf{y}_i \vert \color{blue}{\mathbf{u}_i}\right)f\left(\color{blue}{\mathbf{u}_i}\right)} \mathrm{d} \mathbf{u}_{i}$$`
]


???

which is the product between the conditional pdf or pmf and random effect density
---
class: center, middle

### Likelihood Inference `\(\Rightarrow\)` **(Numerical) Integration** (of `\(\color{\blue}{\mathbf{u}_{i}}\)` )

.content-box-lightgrey[
`$$\color{red}{\mathcal{L}_i\left(\mathbf{\theta}\right)} =\int_{\mathbb{R}^q} \color{red}{\exp} \left[ \color{blue}{\log f\left(\mathbf{y}_i \vert \color{blue}{\mathbf{u}_i}\right)  + \log  f\left(\color{blue}{\mathbf{u}_i}\right)} \right] \ \mathrm{d}\mathbf{u}_{i}$$`
]

???

which can be written for convenience as the exponents in the log 

--

### e.g. via the **Laplace Approximation** `\(\rightarrow \color{red}{\tilde{\mathcal{L}}\left(\mathbf{\theta}\right)}\)`.
---
class: inverse, center, middle

# Bootstrapping Clustered Data

---
class: center, middle

### _Classical_ Bootstrap **does not respect the clustering structure**

&lt;img src="./img/Charts-002.png" width="600"&gt;

---
class: center, middle

### _Cluster_ Bootstrap  **might be limited** by the **number of elements** at each level


&lt;img src="./img/Charts-003.png" width="800"&gt;
---
class: center, middle

### _Residual_ Bootstrap **struggles** with **random effects** and **link function**

--

.pull-left[
Easy(-ish) in **Gaussian LMM**

.content-box-lightgrey[
`$$\mathbf{x}_{ij}^T\widehat{\mathbf{\beta}} + \widehat{\sigma}_1\color{red}{\widehat{u}_{i}^{{\star}}} + \widehat{\phi}\color{red}{\widehat{\varepsilon}_{i}^{{\star}}} \longrightarrow  y_{ij}^{\color{red}{\star}}$$`
]

&lt;img src="https://media.giphy.com/media/4JSAfzWfvMN36r1HG1/giphy.gif" style="width: 200px" /&gt;

]

--
.pull-right[

Not that clear **in other cases**

.content-box-lightgrey[
`$$\mathbf{x}_{ij}^T \widehat{\mathbf{\beta}} + \widehat{\sigma}_1\color{red}{\widehat{u}_{i}^{{\star}}}  \overset{\mathbf{???}}{\rightarrow}
g(\mathbb{E}[y_{ij}^{\color{red}{\star}}]) 
\overset{\mathbf{???}}{\rightarrow}
y_{ij}^{\color{red}{\star}}$$`
]

&lt;img src="https://media.giphy.com/media/xf20D8HzvTQzu/giphy.gif" style="width: 200px" /&gt;

]

---
class: center, middle

background-image: url(./img/meme-lisapresenting.png)

???

At this point in the talk it appears that Parametric Bootstrap (approach based on simulations) is the only feasible and convenient alternative 

---
class: inverse, center, middle

# Bootstrapping Clustered Data via a Weighted Laplace Approximation* 

???

That said, it also help us segue into the title of the talk

--

(*yup! Our proposal!)

???

What is it that we propose?

---
class: dark, center, middle



## Random Weighted Laplace Bootstrap (RWLB)

--

### **General** scheme relying on **externally generated _Random Weights_**

---

# RWLB Algorithm

### 1. **Generate _Random Weights_**  `\({\color{red}{w_{i}^{*}}}\)`
--

### 2. **Apply** to `\(\color{blue}{f(\mathbf{y}_i,\mathbf{u}_i)}\)` 
--

### 3. **Evaluate via the _Laplace Approximation_** `\({\color{red}{\tilde{\mathcal{L}}_{i}^{*}}}\)`  
--

### 4. Optimize
--
...
--

### 5. Profit! 
---

# (Some) Details

### 1. **Generate _Random Weights_**  `\({\color{red}{w_{i}^{*}}}\)`

.content-box-lightgrey[
`$$\mathbb{E}\left[{\color{red}{w^{*}_{i}}}\right] = \mathbb{V}ar\left[{\color{red}{w^{*}_{i}}}\right]=1 \qquad \text{e.g. } \mathcal{E}(1)$$`
]

--

### 2. **Apply** to `\(\color{blue}{f(\mathbf{y}_i,\mathbf{u}_i)}\)` 

.content-box-lightgrey[
`$$\mathcal{L}_i^{*} \left(\mathbf{\theta}\right) = 
\int_{\mathbb{R}^q} 
\exp \left\{- \left[ - {\color{red}{w_{i}^{*}}} 
\color{blue}{\log f\left(\mathbf{y}_i \vert \mathbf{u}_i\right)}  - 
{\color{red}{w_{i}^{*}}} \log f\left(\color{blue}{\mathbf{u}_i}\right)\right] \right\}   \mathrm{d}\mathbf{u}_{i}$$`
]

--

### 3.(4. and 5.)  **Evaluate** : Package `TMB` (`C++` Templates and `R`)

---
class: inverse, center, middle

# (Some) Properties

---
class: dark, center, middle

# The Good <i class="fas  fa-thumbs-up "></i>

![](https://media.giphy.com/media/RXXTZtYKTubiU/giphy.gif)

---
class: middle

# <i class="fas  fa-thumbs-up "></i> Flexible

--

## Can be applied to **a wide variety of Mixed Models**

--

### <i class="fas  fa-arrow-circle-right "></i> Can deal with **Non-Gaussian Responses** and **Censored Data**
  
### <i class="fas  fa-arrow-circle-right "></i> Software **implementation is straightforward**

---
### <i class="fas  fa-thumbs-up "></i> Flexible

.content-box-lightgrey[
`$$\mathcal{L}_i^{*} \left(\mathbf{\theta}\right) = 
\int_{\mathbb{R}^q} 
\exp \left\{- \left[ - {\color{red}{w_{i}^{*}}} 
\color{blue}{\log f\left(\mathbf{y}_i \vert \mathbf{u}_i\right)}  - 
{\color{red}{w_{i}^{*}}} \log f\left(\color{blue}{\mathbf{u}_i}\right)\right] \right\}   \mathrm{d}\mathbf{u}_{i}$$`
]

--

.black[Example:] **Mixed Logit**

.tothe-left[
`template.cpp`: application of `\(\color{red}{w_i^{*}}\)`

```c
...
for(int j=0;j&lt;ngroups;j++){
* res-= w[j]*dnorm(u[j], Type(0) ,sigma,1); 
}

for(int i=0;i&lt;nobs;i++){
  j=group[i];
* res-= w[j]*dbinom(y[i],Type(1),Type(1/(1+exp(-mu[i]-u[j]))),1);
}
...
```
]
--
.tothe-right[

`estimation.R`: Approximation and `\(\mathcal{L}(\mathbf{\theta})\)`
```r
...
# Compilation
* compile("template.cpp")

# Loading to the Environment
dyn.load(dynlib("template")) 

# AD Object creation
obj &lt;- MakeADFun(data=data,
                 parameters=parameters,
                 random="u",
                 DLL="template"
                 )
# Optimisation                  
opt &lt;-nlminb(obj$par, obj$fn, obj$gr)
...
```
]

---
class: middle

# <i class="fas  fa-thumbs-up "></i> Correct

--

## **Good approximation** of the **Asymptotic Distribution**

--

### <i class="fas  fa-arrow-circle-right "></i> `\(\sqrt{n} \left(\widehat{\mathbf{\beta}}^{*} - \widehat{\beta}\right)\)` is <i class="fas  fa-check "></i>

--

### <i class="fas  fa-arrow-circle-right "></i>  `\(\sqrt{n} \left(\widehat{\mathbf{\sigma}}^{*} - \widehat{\sigma}\right)\)` is <i class="fas  fa-check-circle "></i> 

---
class: dark, middle, center 

# Simulations
--

# 1. **Mixed Logit** 
---

### Simulation: **Mixed Logit**

- **Parameters** :  .black[4 Fixed Effects] ( `\(\beta_k\)` ) and .black[1 Random intercept] ( `\(\sigma_1\)` )

- **Number of Simulations**: 1000

- **Studied methods**: .green[RWLB] vs .red[Parametric Bootstrap (PB)]

- **Cluster size** ( `\(n_i\)` ): **7** and **21**

- **Number of clusters** ( `\(n\)` ): **300** and **600**

--

### .black[Q-Q plots]: _Average_ **Bootstrap** replicates vs. **Empirical** Distributions 

.content-box-lightgrey[
`$$\sqrt{n}\left(\widehat{\beta}_{k}^{*}-\widehat{\beta}_{k}\right) \ \  \text{vs.} \ \ \sqrt{n}\left(\widehat{\beta}_{k} - \beta_{k}\right)
\qquad \sqrt{n} \left(\widehat{\sigma}_1^{*} - \widehat{\sigma}_{1}\right) \ \ \text{vs.} \ \ \sqrt{n} \left(\widehat{\sigma}_{1} - \sigma_1\right)$$`
]

???

To illustrate these points, let me show you a comparison of the Mean Bootstrap distribution vs. the empirical by means of qq-plots

---
class: center

###  Fixed Effect Parameters `\(\beta_k\)`
**Number of clusters** `\(n = 300\)`
.pull-left[
**Cluster size** `\(n_i= 7\)`
&lt;img src="index_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;
]

???

As we have said at the introduction of this section, the approximation of the bootstrap to the asymptotic distribution is very correct as shown by the alignment of the green lines over the identiy line.

This is valid accross 

--

.pull-right[
**Cluster size** `\(n_i = 21\)`
&lt;img src="index_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;
]

---
class: center

### Random Effect Variance `\(\sigma_1^{2}\)`


**Cluster size** `\(n_{i} = 7\)`
.pull-left[
**Number of Clusters:** `\(n = 300\)`
&lt;img src="index_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
**Number of Clusters:** `\(n = 600\)`
&lt;img src="index_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;
]

---
class: center

### Random Effect Variance `\(\sigma_{1}^{2}\)`
**Cluster size** `\(n_{i} = 14\)`
.pull-left[
**Number of Clusters:** `\(n = 300\)`
&lt;img src="index_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
**Number of Clusters:** `\(n = 600\)`
&lt;img src="index_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;
]

---
class: center

### Random Effect Variance `\(\sigma_{1}^{2}\)`
**Cluster size** `\(n_{i} = 21\)`


.pull-left[
**Number of Clusters:** `\(n = 300\)`
&lt;img src="index_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
**Number of Clusters:** `\(n = 600\)`
&lt;img src="index_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;
]
---
class: dark, middle, center 

# Simulations
--

# 2. **Log-Normal AFT** 
---
### Simulated **Log-Normal AFT**

- **Parameters**: 4 Fixed Effects ( `\(\beta_k\)` ), 1 Random intercept ( `\(\sigma_1\)` ) and a Scale ( `\(\phi\)` ) 

- Number of Simulations: 1000

- **Studied methods**: .green[RWLB] vs .pink[Asymptotic Approximation] 

- **Cluster sizes ( `\(n_i\)` )**: 4, 10, 20, 50

- **Number of clusters ( `\(n\)` )**: 20, 50, 100, 200 

- **Degree of censoring**:  0% and 23%

- **Signal-to-noise Ratio** `\(\displaystyle \frac{\sigma_{1}}{\phi} = 1/0.5 = 2\)`

--

###  .black[Box-Plots]: Small-sample Bias Correction of `\(\widehat{\sigma_1}\)`

.content-box-lightgrey[
`$$\widehat{\sigma}_1^{\text{BC}} = \widehat{\sigma}_1^{\text{ML}} - \frac{1}{B}\sum_{b=1}^B\left[ \color{forestgreen}{\widehat{\sigma}^{*}_{1}} -  \widehat{\sigma}_1^{\text{ML}}\right]$$`
]

---
class: center

### Boxplots of `\(\widehat{\sigma}_1\)`



&lt;img src="index_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

---
class: center

### Boxplots of `\(\widehat{\sigma}_1\)`

&lt;img src="index_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;


---
class: center

### Boxplots of `\(\widehat{\phi}\)`

&lt;img src="index_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;

---
class: center

### Boxplots of `\(\widehat{\phi}\)`

&lt;img src="index_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

---
class: dark, center, middle

# The 'meh' <i class="fas  fa-thumbs-down "></i>

![](https://media.giphy.com/media/QC5jE83f3tBCg/giphy.gif)

---
# Some Caveats

### <i class="fas  fa-thumbs-down "></i> Can be **unstable** in **extreme Cases**

--

- RWLB is tributary to the quality of the estimation method.

    - **Cluster Size** is **too small** and/or

    - **Signal-to-noise** ratio is **weak** 

---
class: center

### Simulated AFT - `\(\widehat{\sigma}_1\)` : Same as before

&lt;img src="index_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---
class: center

### Simulated AFT - `\(\widehat{\sigma}_1\)` : **s-t-n = 1**

&lt;img src="index_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;
---
class: center

### Simulated AFT - `\(\widehat{\sigma}_1\)` : **s-t-n = 0.4**

&lt;img src="index_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;


---
class: center

### Simulated AFT - `\(\widehat{\sigma}_1\)` : **s-t-n = 0.4, `\(n_{i}=2\)`**

&lt;img src="index_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

---
# Some Caveats

### <i class="fas  fa-thumbs-down "></i> **Variance** of some `\(\widehat{\mathbf{\sigma}}^{*}\)`  **differs slightly** from the asymptotic.

- Divergence Depends on the relationship between:  
    
    - **Marginal Variance** of the Observations  `\(\mathbb{V}ar(\mathbf{Y}_i) = \mathbf{\Sigma}_{i}\)`: and 
    
    - **Contrasts of the Random Effects**:  `\(\mathbf{Z}_{i}\)` 

- **In LMM**, this is _easy-ish_ to figure out.
  
    - Relationship **governed by** `\(\text{tr}(\mathbf{\Sigma}_{i}^{-1}\mathbf{Z}_{i}\mathbf{Z}_{i}^{T})\)`

    - **"Fuller" designs**, e.g. *Random Intercept*  `\(\mathbf{Z}_{ir} = \mathbb{1}_{n_{i}}\)` and *Slope*  `\(\mathbf{Z}_{ir} = \mathbf{x}^(k)_{i}\)` seem less affected. 
    
    - **"Sparser"" designs** e.g. the *Random Error* `\(\varepsilon\)` with `\(\mathbf{Z}_{i0} = \mathbf{I}_{n_{i}}\)` seem to be **most divergent**.
    
    - Possible **fix**
    
---
class: inverse, center, middle

# Discussion and Current Work

---

# Discussion

### RWLB has a lot of potential as a bootstrapping method.

--

- <i class="fas  fa-thumbs-up "></i> **Flexible** to be applied in many fields.

- <i class="fas  fa-thumbs-up "></i> **Correct** to provide good inference.

--

### RWLB has a few drawbacks that need exploration.

- <i class="fas  fa-thumbs-down "></i> **settings** where it appears to fail

- <i class="fas  fa-thumbs-down "></i> **improvements** to its variability

---

# Current Work

### 1. Data applications 

- Time-to-failure studies on patients submitting to dyalisis under two different treatments.

--

### 2. Further software implementation of estimation methods in AFT


- `R Package` allowing for flexible estimation of AFT models for Clustered Data (different distributions for durations and Random effects)

- Bootstrapping via RWLB is intended as a feature. 

--

### 3. Improvements

- REML-like-based estimation method and its effects on variance and bias. 

---

# References (and Associated Publications) 

- F-A, Cantoni (2020),  **Bootstrapping generalized linear mixed models via a weighted Laplace approximation**, 

    -   _Under Review_

-  F-A, Cantoni (2019), **Bootstrap estimation of uncertainty in prediction for generalized linear mixed models**, 

    - _Computational Statistics &amp; Data Analysis_, Volume 130, 2019, Pages 1-17 
    - <i class="ai  ai-doi "></i> [10.1016/j.csda.2018.08.006](10.1016/j.csda.2018.08.006)


---
class: center, middle

# Questions?

---
class: center, middle

# Remarks?
 
---
class: center, middle

# I'll stick around!

---
class: center, middle

# Thanks!

Slides created via the `R` package [**xaringan**](https://github.com/yihui/xaringan).

Research funded by the
National Health and Medical Research Council (NHMRC) of Australia (APP 1128222)

Travel Grant accorded by ViCBiostats and Monash University, Melbourne. 

]

---

class: inverse, center, middle

# Appendix

---
class: dark , center, middle

# (Some) Theory

---
# Comparison with GCB*
 
**Generalised Cluster Bootstrap** for Gaussian LMM (Field, Pang, Welsh 2014)  

`$$\widehat{\mathbf{\theta}}^{*} = \underset{\theta}{\mathrm{arg}} \left\{ \frac{1}{\sqrt{n}}\sum_{i=1}^{n} \color{red}{w_{i}^{*}} \Psi_{i}\left(\mathbf{\theta}\right) = \mathbf{0} \right\}$$`

--
.tothe-left[
.content-box-lightgrey[
.black[Theorem:] Under regularity conditions on `\(\Psi_i\left(\mathbf{\theta}\right)\)`, ensuring Asymptotic Normality i.e.

`$$\sqrt{n}\left(\widehat{\mathbf{\theta}}-\mathbf{\theta}\right)
\overset{\mathcal{D}}{\underset{n\rightarrow \infty}{\longrightarrow}}
\mathcal{N}\left\{ \mathbf{0},\mathbf{M}^{-1}\mathbf{Q}\mathbf{M}^{-1}\right)$$`

for fixed `\(n_i\)`, then:
.content-box-fuchsia[
.white[
`$$\sqrt{n}\left(\widehat{\mathbf{\theta}}^{*}- \widehat{\mathbf{\theta}}\right)\overset{\mathcal{D}}{\underset{n\rightarrow\infty}{\longrightarrow}}\mathcal{N}\left(\mathbf{0},\widehat{\mathbf{M}}^{-1}\widehat{\mathbf{Q}}\widehat{\mathbf{M}}^{-1}\right)$$`]
]
]
]
--
.tothe-right[

`\(\widehat{\mathbf{M}}^{-1}\widehat{\mathbf{Q}}\widehat{\mathbf{M}}^{-1}\)` is the _Empirical_ **Sandwich Variance**

&lt;img src="https://media.giphy.com/media/jOuA5oDxeIWJi/giphy.gif" style="width: 150px" /&gt;

]

---

Elements are **Expectations of products of `\(\Psi^{(k)}_i\left(\mathbf{\theta}_0\right)\)`** or its **Gradient**:

`\(\forall k, l \in \{1, \dots d\}\)`, where `\(d = p + s\)`,  where `\(\mathbf{M} = \left( M_{kl}\right)_{k=1, l=1}^{p,s}\)`, `\(\mathbf{Q} = \left( Q_{kl}\right)_{k=1, l=1}^{p,s}\)`

**Asymptotic Variance**
.content-box-lightgrey[
`\begin{align}
-\frac{1}{n}
\sum_{i=1}^n 
\mathbb{E} 
\left\{ 
{\color{blue}{\partial_{\theta_{l}}\Psi^{(k)}_i\left(\mathbf{\theta}_0\right)}}
\right\}
&amp;\underset{n\rightarrow \infty}{\longrightarrow}
M_{kl}\\
\frac{1}{\sqrt{n}}\sum_{i=1}^n 
\mathbb{E} 
\left\{ 
{\color{blue}{\Psi^{(k)}_i\left(\mathbf{\theta}_0\right)
\Psi^{(l)}_i\left(\mathbf{\theta}_0\right)}}
\right\} 
&amp;\underset{n\rightarrow \infty}{\longrightarrow}
Q_{kl}
\end{align}`
]


--

**GCB** version: 
.content-box-lightgrey[

`\begin{align}
-\frac{1}{n}
\sum_{i=1}^n 
\mathbb{E}^{*}\left[\color{red}{w_{i}^{*}}\right]  
\left\{ 
\partial_{\theta_{l}}\Psi^{(k)}_i\left(\mathbf{\theta}_0\right)
\right\}
&amp;\underset{n\rightarrow \infty}{\longrightarrow}
M_{kl}
\end{align}`

`\begin{align}
\frac{1}{n}
&amp;\sum_{i=1}^n 
\mathbb{V}ar \left[ w_{i}^{*}\right]
\Psi^{(k)}_i(\widehat{\mathbf{\theta}})
\Psi^{(l)}_i(\widehat{\mathbf{\theta}}) \nonumber \\
&amp;+ 
\frac{1}{n}\sum_{i=1}^n\sum_{j\neq i}
\mathbb{C}ov \left[ w_{i}^{*}, w_{j}^{*}\right]
\left\{ 
\Psi^{(k)}_i(\widehat{\mathbf{\theta}})
\Psi^{(l)}_j(\widehat{\mathbf{\theta}})
\right\}  
\underset{n\rightarrow \infty}{\longrightarrow}
Q_{kl}
\end{align}`
]


---

# GCB vs RWLB in _Random Effect_ LMM

`$$\mathbf{y}_i = \mathbf{X}_{i}\mathbf{\beta} + \sum_{r=0}^{q} \sigma_r \mathbf{Z}_{ir}\mathbf{u}_{ir} \longrightarrow \begin{array}{l c l} \tilde{\Psi}^{\beta}_i\left(\mathbf{\theta}\right)
  &amp;=&amp;\mathbf{X}_i^T \mathbf{\Sigma}_i^{-1} \left(\mathbf{y}_i - \mathbf{X}_i\mathbf{\beta}\right),
  \\
  \tilde{\Psi}^{\sigma^{2}_{r}}_i\left(\mathbf{\theta}\right)
  &amp;=&amp;
  -\frac{1}{2}
  \left( \mathbf{y}_i-\mathbf{X}_i\mathbf{\beta}\right)^T
  \mathbf{\Sigma}_i^{-1}\mathbf{Z}_{ir}\mathbf{Z}_{ir}^T\mathbf{\Sigma}_i^{-1}
  \left(\mathbf{y}_i-\mathbf{X}_i\mathbf{\beta}\right)  + \frac{1}{2}
  \mathrm{tr}\left( \mathbf{\Sigma}_{i}^{-1} \mathbf{Z}_{ir} \mathbf{Z}_{ir}^T\right). \end{array}$$`

--

.pull-left[
**GCB**
`\begin{align}
  \tilde{\Psi}^{\beta}_i\left(\mathbf{\theta}\right)
  &amp;=\color{red}{w_{i}^{*}}\mathbf{X}_i^T \mathbf{\Sigma}_i^{-1} \left(\mathbf{y}_i - \mathbf{X}_i\mathbf{\beta}\right),
  \\
  \tilde{\Psi}^{\sigma^{2}_{r}}_i\left(\mathbf{\theta}\right)
  &amp;=
  -\frac{\color{red}{w_{i}^{*}}}{2}
  \left( \mathbf{y}_i-\mathbf{X}_i\mathbf{\beta}\right)^T
  \mathbf{\Sigma}_i^{-1}\mathbf{Z}_{ir}\mathbf{Z}_{ir}^T\mathbf{\Sigma}_i^{-1}
  \left(\mathbf{y}_i-\mathbf{X}_i\mathbf{\beta}\right)  \\
  &amp;+ 
  \frac{\color{red}{w_{i}^{*}}}{2}
  \color{blue}{\mathrm{tr}\left( \mathbf{\Sigma}_{i}^{-1} \mathbf{Z}_{ir} \mathbf{Z}_{ir}^T\right).} 
\end{align}`
]
.pull-right[
**RWLB** (after tedious derivation)

.content-box-lightgrey[
`\begin{align}
  \tilde{\Psi}^{\beta}_i\left(\mathbf{\theta}\right)
  &amp;=\color{red}{w_{i}^{*}}\mathbf{X}_i^T \mathbf{\Sigma}_i^{-1} \left(\mathbf{y}_i - \mathbf{X}_i\mathbf{\beta}\right),
  \\
  \tilde{\Psi}^{\sigma^{2}_{r}}_i\left(\mathbf{\theta}\right)
  &amp;=
  -\frac{\color{red}{w_{i}^{*}}}{2}
  \left( \mathbf{y}_i-\mathbf{X}_i\mathbf{\beta}\right)^T
  \mathbf{\Sigma}_i^{-1}\mathbf{Z}_{ir}\mathbf{Z}_{ir}^T\mathbf{\Sigma}_i^{-1}
  \left(\mathbf{y}_i-\mathbf{X}_i\mathbf{\beta}\right)  \\
  &amp;+ 
  \color{blue}{\frac{1}{2}
  \mathrm{tr}\left( \mathbf{\Sigma}_{i}^{-1} \mathbf{Z}_{ir} \mathbf{Z}_{ir}^T\right).} 
\end{align}`
]
]


  


---
class: center
.pull-left[
![](index_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;
]
---
class: center
.pull-left[
![](index_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;
] 

---
class: center

.pull-left[
![](index_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;
]
---
class: center

.pull-left[
![](index_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;
]

---


.pull-left[
![](index_files/figure-html/unnamed-chunk-33-1.png)&lt;!-- --&gt;
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;
]

---
class: center

.pull-left[
![](index_files/figure-html/unnamed-chunk-35-1.png)&lt;!-- --&gt;
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-36-1.png)&lt;!-- --&gt;
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": true
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
